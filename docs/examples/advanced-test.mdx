---
title: 'Advanced Testing Example'
description: 'Learn how to create more sophisticated tests with Scenario'
---

# Advanced Testing Example

This example demonstrates more advanced testing techniques with Scenario, including custom testing agents, multi-turn conversations, and artifact collection.

## Multi-turn Conversation Testing

```python
import scenario
from scenario import Scenario, TestingAgent

# Define a more sophisticated agent function
def math_tutor_agent(message, context=None):
    """A math tutor agent that can solve problems and explain concepts."""
    context = context or {}
    history = context.get("history", [])
    history.append({"role": "user", "content": message})
    
    # Simple logic to simulate a math tutor
    if "solve" in message.lower() and any(op in message for op in ["+", "-", "*", "/"]):
        # Extract the math problem and solve it
        # This is a simplified example - a real agent would have more sophisticated parsing
        response = "Let me solve that for you. The answer is 42. Here's how I approached it..."
    elif "explain" in message.lower():
        response = "I'd be happy to explain that concept. In mathematics, this works by..."
    else:
        response = "I'm your math tutor assistant. You can ask me to solve problems or explain concepts."
    
    history.append({"role": "assistant", "content": response})
    
    # Return both the message and the updated history as an artifact
    return {
        "message": response,
        "history": history,
        "thinking": "This is my internal reasoning process for solving this problem..."
    }

# Create a custom testing agent with specific configuration
custom_testing_agent = TestingAgent(
    custom_config={
        "model": "gpt-4",
        "temperature": 0.2,
        "max_tokens": 2000
    }
)

# Create a more complex scenario
math_tutoring_test = Scenario(
    description="Test if the math tutor agent can correctly solve problems and explain concepts",
    success_criteria=[
        "Agent correctly solves the math problem",
        "Agent explains the solution process",
        "Agent maintains a helpful and educational tone"
    ],
    failure_criteria=[
        "Agent provides incorrect answers",
        "Agent fails to explain the solution process",
        "Agent is dismissive or unhelpful"
    ],
    agent=math_tutor_agent,
    testing_agent=custom_testing_agent,
    strategy="Ask the agent to solve a math problem first, then ask it to explain the concept behind the solution.",
    max_turns=5
)

# Run the test
result = math_tutoring_test.run(context={"initial_prompt": "I need help with algebra"})

# Analyze the results
print(f"Test passed: {result.success}")

# Access conversation history
for message in result.conversation:
    print(f"{message['role']}: {message['content']}")

# Access artifacts collected during the test
if "thinking" in result.artifacts:
    print(f"Agent's internal reasoning: {result.artifacts['thinking']}")

if "history" in result.artifacts:
    print(f"Conversation history length: {len(result.artifacts['history'])}")
```

## Testing with Custom Evaluation Logic

You can also implement custom evaluation logic for your tests:

```python
from scenario import Scenario, TestingAgent, ScenarioResult

class CustomTestingAgent(TestingAgent):
    """A custom testing agent with specialized evaluation logic."""
    
    def evaluate_response(self, response, scenario):
        """Custom evaluation logic for agent responses."""
        # Implement your own evaluation logic here
        # For example, you might use a specialized model or heuristics
        
        # Check for mathematical accuracy
        contains_numbers = any(char.isdigit() for char in response)
        contains_explanation = len(response) > 100  # Simple heuristic
        
        if contains_numbers and contains_explanation:
            return ScenarioResult.success_result(
                conversation=self._conversation,
                artifacts=self._artifacts,
                met_criteria=["Contains numerical answer", "Provides explanation"]
            )
        else:
            return ScenarioResult.failure_result(
                conversation=self._conversation,
                artifacts=self._artifacts,
                failure_reason="Response missing numbers or explanation",
                triggered_failures=["Missing numerical answer" if not contains_numbers else None,
                                   "Missing explanation" if not contains_explanation else None]
            )

# Use the custom testing agent
custom_agent = CustomTestingAgent()
result = math_tutoring_test.run(testing_agent=custom_agent)
```

## Integration with External Evaluation Tools

You can integrate Scenario with external evaluation tools:

```python
import json
from scenario import Scenario

def evaluate_with_external_tool(conversation):
    """Simulate using an external evaluation tool."""
    # In a real implementation, this might call an API or another service
    return {
        "accuracy": 0.92,
        "helpfulness": 0.85,
        "toxicity": 0.02
    }

# Run your scenario test
result = math_tutoring_test.run()

# Evaluate the results with an external tool
if result.success:
    external_evaluation = evaluate_with_external_tool(result.conversation)
    
    # Save the evaluation as JSON
    with open("evaluation_results.json", "w") as f:
        json.dump({
            "scenario_result": result.to_dict(),
            "external_evaluation": external_evaluation
        }, f, indent=2)
    
    print(f"Accuracy score: {external_evaluation['accuracy']}")
    print(f"Helpfulness score: {external_evaluation['helpfulness']}")
```
